{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1208f03-60be-406f-8b15-5a474895e68d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Second-order Optimization\n",
    "\n",
    "To overcome the issues of vanilla gradient descent we introduce Newton's method, a second-order optimization scheme that exploits second-order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ded2bc-2cb5-4f33-a1cf-5fd588f6d3f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimality Conditions\n",
    "\n",
    "Given an unconstrained optimization problem:\n",
    "$$\n",
    "\\min_x f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554f89c-c7a1-4a10-929a-168dcfb81182",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A local minimizer of $f(x)$ must satisfy the following condition:\n",
    "$$\n",
    "\\nabla f(x) = 0\n",
    "$$\n",
    "This condition is **necessary**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72634472-3b11-407c-8f46-2a644feb42b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Moreover, a **sufficient** (but not necessary) condition for a local minimizer is:\n",
    "$$\n",
    "\\nabla^2 f(x) > 0\n",
    "$$\n",
    "which means that the Hessian $\\nabla^2 f(x)$ is positive definite (i.e. all its eigenvalues are positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e054ba-e26e-4527-9ff8-7136cfef97d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Newton's Method\n",
    "\n",
    "The necessary optimality conditions define a nonlinear system of equations:\n",
    "$$\n",
    "r(x) \\triangleq \\nabla f(x) = 0\n",
    "$$\n",
    "We can try to solve this system of equations with Newton's method, an iterative algorithm that produces successively better approximations to the zeroes of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adaa1f3-35cc-48c6-a887-91f4ce183522",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Newton's method approximates the function $r(x)$ in the neighborhood of the current guess $\\bar{x}$ using a first-order Taylor's series expansion:\n",
    "$$\n",
    "r(\\bar{x} + \\Delta x) \\approx r(\\bar{x}) + \\nabla r(\\bar{x})^T \\, \\Delta x = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b27174-be9f-43ff-8d44-8f16d3990900",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can solve this linear system of equations by computing the so-called \"Newton's step\":\n",
    "$$\n",
    "\\Delta x = - \\nabla r(\\bar{x})^{-T} \\, r(\\bar{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc7bde-dd4e-45a0-b7c0-c4ffd8cce6ce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we update our guess by applying Newton's step:\n",
    "$$\n",
    "\\bar{x} := \\bar{x} + \\Delta x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17b7e5-c255-4920-8d95-d38f3886523c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We repeat this process until $||r(x)|| \\le \\text{threshold}$. Let's see an implementation of Newton's method for solving the equation $x^4 - x = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f47511-513e-4211-b177-46dd17337a80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# the system of equations to solve is defined as: r(x)=0\n",
    "def r(x):\n",
    "    return x**4 -x\n",
    "\n",
    "# function computing the gradient of r(x)\n",
    "def grad_r(x):\n",
    "    return 4*(x**3) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "426bc23b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def newtons_method_root_finding(n_iters):\n",
    "    # n_iters: number of iterations to perform\n",
    "    x_bar = -2.0 # initial guess\n",
    "    \n",
    "    # plot the function r(x)\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    X = np.linspace(-2.1, 0.1, 100)\n",
    "    plt.plot(X, r(X), linewidth=3, label=\"r(x)\")\n",
    "\n",
    "    # perform n_iters iterations of Newton's method\n",
    "    plt.plot(x_bar, r(x_bar), 'o', label=\"Initial guess\")\n",
    "    for i in range(n_iters):\n",
    "        # at the last iteration plot the Taylor expansion of r(x) around x_bar\n",
    "        if(i==n_iters-1):\n",
    "            plt.plot(X, r(x_bar) + grad_r(x_bar)*(X-x_bar), alpha=0.5, linewidth=2, label=\"Taylor expansion\")\n",
    "            \n",
    "        # compute Newton's step\n",
    "        delta_x = - r(x_bar) / grad_r(x_bar)\n",
    "        x_bar += delta_x\n",
    "        plt.plot(x_bar, r(x_bar), 'o', label=\"Iteration \"+str(i+1))\n",
    "        if(i==n_iters-1):\n",
    "            plt.axvline(x=x_bar, ymin=1.0/23, ymax=(r(x_bar)+1)/23, ls=':', color='r', linewidth=2, alpha=0.5)\n",
    "\n",
    "    print(\"Final value of r(x)=\", r(x_bar))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylim([-1, 22])\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.axvline(x=0, color='k')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7a45003",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecf449bc8eb459ab4ec1d3c39a19176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='num iters', max=7, min=1), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slider = IntSlider(value=1, min=1, max=7, step=1, description='num iters')\n",
    "interact(newtons_method_root_finding, n_iters=slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9dbd8d-6d24-4c02-b0d3-2afe30a2eea0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Newton's method for solving minimization problems\n",
    "\n",
    "Let us now suppose that the system of equations that we want to solve with Newton's method is:\n",
    "$$\n",
    "\\nabla f(x) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc0048-f9c0-4f91-99ff-b4d9bb8fcb02",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Newton's step then takes the form:\n",
    "$$\n",
    "\\Delta x = - \\nabla^2 f(\\bar{x})^{-1} \\, \\nabla f(\\bar{x})\n",
    "$$\n",
    "Contrary to gradient descent, Newton's step exploits the Hessian of the function $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bca0f2-5842-4778-801c-d10c66948e99",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Alternative Interpretation\n",
    "Another way to interpret Newton's method is that at each step it minimizes a local 2-nd order approximation of $f(x)$:\n",
    "$$ \\min_{\\Delta x} \\, f(\\bar{x}) + \\nabla f(\\bar{x})^T \\, \\Delta x + \\frac{1}{2} \\Delta x^T \\, \\nabla^2 f(\\bar{x}) \\, \\Delta x $$\n",
    "which results in the same Newton's step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0411654-57ae-46df-ada9-a2fd6bc05f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "Let's try to minimize the same ill-conditioned function that was leading to slow convergence with gradient descent.\n",
    "$$ f(x, y) = 100x^2 + y^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19814c51-7b19-4a2d-9860-6a071b524954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv, norm\n",
    "\n",
    "# Solve the minimization problem: min f(x)\n",
    "def f_ill(x):\n",
    "    return 100*x[0]**2 + x[1]**2\n",
    "\n",
    "def f_ill_plot(x, y):\n",
    "    return f_ill([x,y])\n",
    "\n",
    "# function computing the gradient of f(x)\n",
    "def grad_f_ill(x):\n",
    "    return np.array([200*x[0], 2*x[1]])\n",
    "\n",
    "# function computing the Hessian of f(x)\n",
    "def hess_f_ill(x):\n",
    "    return np.array([[200, 0],\n",
    "                     [0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4595642b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def newtons_method_nd(n_iters, x_bar):\n",
    "    # perform n_iters iterations of Newton's method\n",
    "    history_x = [x_bar.copy()]\n",
    "    history_f = [f_ill(x_bar)]\n",
    "    for i in range(n_iters):\n",
    "        # compute Newton's step\n",
    "        delta_x = - inv(hess_f_ill(x_bar)) @ grad_f_ill(x_bar)\n",
    "        x_bar += delta_x\n",
    "        # store the updated value of x_bar and f(x_bar)\n",
    "        history_x.append(x_bar.copy())\n",
    "        history_f.append(f_ill(x_bar))\n",
    "\n",
    "    print(\"Final value of x =  \", x_bar)\n",
    "    print(\"Final value of f(x)=\", f_ill(x_bar))\n",
    "    print(\"Final value of the gradient of f(x)=\", norm(grad_f_ill(x_bar)))\n",
    "\n",
    "    # plot the function f(x)\n",
    "    X, Y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\n",
    "    Z = f_ill_plot(X, Y)\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.subplots(1,2)\n",
    "    ax[0].contour(X, Y, Z, levels=np.logspace(-1, 3, 20))\n",
    "    history_x = np.array(history_x)\n",
    "    ax[0].plot(history_x[:,0], history_x[:,1], 'ro-', label='Trajectory', alpha=0.5)\n",
    "    ax[0].set_xlabel(\"x[0]\"), ax[0].set_ylabel(\"x[1]\")\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].plot(history_f)\n",
    "    ax[1].set_ylabel(\"Cost function\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33354ee4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c414db62b844e68b4218add7388a9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='num iters', max=5, min=1), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def newtons_method_0(n_iters):\n",
    "    return newtons_method_nd(n_iters, x_bar=np.array([1.5, 1.5]))\n",
    "interact(newtons_method_0, n_iters=IntSlider(value=1, min=1, max=5, step=1, description='num iters'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a5a40-fd3a-4ded-ac4e-0198c9a70946",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Another example\n",
    "Let's try to minimize another function:\n",
    "$$f(x) = x^4 - x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71f08fb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Solve the minimization problem: min f(x)\n",
    "def f(x):\n",
    "    return x**4 -x\n",
    "\n",
    "# function computing the gradient of f(x)\n",
    "def grad_f(x):\n",
    "    return 4*(x**3) - 1\n",
    "\n",
    "# function computing the Hessian of f(x)\n",
    "def hess_f(x):\n",
    "    return 12*(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4e39ae9-15f3-455f-8b0e-b5206ad70b02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def newtons_method_1d(n_iters, x_bar, ylim):\n",
    "    # plot the function f(x)\n",
    "    X = np.linspace(-2.1, 3.5, 100)\n",
    "    fig = plt.figure(figsize=(14,6))\n",
    "    plt.plot(X, f(X), linewidth=3, label=\"f(x)\")\n",
    "\n",
    "    # perform n_iters iterations of Newton's method\n",
    "    plt.plot(x_bar, f(x_bar), 'o', label=\"Initial guess\")\n",
    "    for i in range(n_iters):\n",
    "        if(i==n_iters-1):\n",
    "            # plot 2-nd order Taylor expansion of f(x) around x_bar\n",
    "            plt.plot(X, f(x_bar) + grad_f(x_bar)*(X-x_bar) + 0.5*hess_f(x_bar)*((X-x_bar)**2), alpha=0.5, linewidth=2)\n",
    "        # compute Newton's step\n",
    "        delta_x = - grad_f(x_bar) / hess_f(x_bar)\n",
    "        x_bar += delta_x\n",
    "        # plot the updated value of x_bar\n",
    "        plt.plot(x_bar, f(x_bar), 'o', label=\"Iteration \"+str(i+1))\n",
    "    plt.axvline(x=x_bar, ls=':', color='r', linewidth=2, alpha=0.5)\n",
    "\n",
    "    print(\"Final value of x =  \", x_bar)\n",
    "    print(\"Final value of f(x)=\", f(x_bar))\n",
    "    print(\"Final value of the gradient of f(x)=\", grad_f(x_bar))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylim(ylim)\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.axvline(x=0, color='k')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3e490fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd91eb01435249419436d3b1dd900c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='num iters', max=12, min=1), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def newtons_method_1(n_iters):\n",
    "    return newtons_method_1d(n_iters, x_bar=-2.0, ylim=[-4, 22])\n",
    "interact(newtons_method_1, n_iters=IntSlider(value=1, min=1, max=12, step=1, description='num iters'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb8f37-46ef-4dd0-b581-72f82a912c2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limitations of Vanilla Newton's Method\n",
    "* In the example above, at the 5-th iteration the value of $f(x)$ increased from -0.16 to 91.\n",
    "* In general Newton's method cannot guarantee monotonic improvement, nor convergence.\n",
    "* For example trying to minimize $f(x) = \\frac{1}{4} x^4 - x^2 + 2x$, starting from $\\bar{x}\\approx1$ the method oscillates forever.\n",
    "    * $\\nabla f(x) = x^3 − 2x + 2$\n",
    "    * $\\nabla^2 f(x) = 3 x^2 - 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1270a149-6cf6-4f8f-a9ad-113638724843",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Solve the minimization problem: min f(x)\n",
    "def f(x):\n",
    "    return 0.25*(x**4) - x**2 + 2*x\n",
    "\n",
    "# function computing the gradient of f(x)\n",
    "def grad_f(x):\n",
    "    return x**3 - 2*x + 2\n",
    "\n",
    "# function computing the Hessian of f(x)\n",
    "def hess_f(x):\n",
    "    return 3* (x**2) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3843670f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c511749003462b9cfbe26666405d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='num iters', max=10, min=1), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def newtons_method_2(n_iters):\n",
    "    return newtons_method_1d(n_iters, x_bar=0.99, ylim=[-5,3])\n",
    "    \n",
    "interact(newtons_method_2, n_iters=IntSlider(value=1, min=1, max=10, step=1, description='num iters'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e23691-5d0a-495d-ae0f-0dce7845e0b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducing the line search\n",
    "* To make Newton's method more robust we can introduce a **line search**\n",
    "* Instead of taking a full Newton's step we can take a fraction $\\alpha \\in [0, 1]$ of it:\n",
    "    * $x^{(i+1)} := x^{(i)} + \\alpha \\Delta x$\n",
    "* Different ways to choose $\\alpha$\n",
    "* At the very least we want to ensure monotonic improvement: $f(x^{(i+1)}) < f(x^{(i)})$\n",
    "* Even better, we may require a minimum improvement (Armijo's condition):\n",
    "    * $f(x^{(i+1)}) \\le f(x^{(i)}) - \\gamma \\, \\alpha \\, \\nabla f^T \\, \\Delta x$\n",
    "    * where $\\gamma \\in [0,1]$ is a user-defined hyper-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0e3a4-cf0a-493d-b1f0-1ea322c0a38f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backtracking Line Search\n",
    "* Start with $\\alpha=1$\n",
    "* Compute Newton's step $\\Delta x$\n",
    "* Compute new value of $x$\n",
    "* As long as the chosen decrease condition is not satisfied:\n",
    "    * reduce $\\alpha$\n",
    "    * Compute new value of $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "706e5649-9cde-47dd-9000-9f8f12f8320d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def newtons_method_w_line_search(n_iters, x_bar, ylim):\n",
    "    # plot the function f(x)\n",
    "    X = np.linspace(-2.1, 3.5, 100)\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    plt.plot(X, f(X), linewidth=3, label=\"f(x)\")\n",
    "\n",
    "    # perform n_iters iterations of Newton's method\n",
    "    plt.plot(x_bar, f(x_bar), 'o', label=\"Initial guess\")\n",
    "    for i in range(n_iters):\n",
    "        if(i==n_iters-1):\n",
    "            # plot 2-nd order Taylor expansion of f(x) around x_bar\n",
    "            plt.plot(X, f(x_bar) + grad_f(x_bar)*(X-x_bar) + 0.5*hess_f(x_bar)*((X-x_bar)**2), alpha=0.5, linewidth=2)\n",
    "        # compute Newton's step\n",
    "        delta_x = - grad_f(x_bar) / hess_f(x_bar)\n",
    "        # implement line search\n",
    "        alpha = 1.0\n",
    "        x_try = x_bar + alpha*delta_x\n",
    "        while( f(x_try)>= f(x_bar)):\n",
    "            # reduce alpha\n",
    "            alpha *= 0.5\n",
    "            # compute new value of x\n",
    "            x_try = x_bar + alpha*delta_x\n",
    "            # stop line search if alpha is too small\n",
    "            if(alpha<1e-4):\n",
    "                break\n",
    "        x_bar = x_try\n",
    "        \n",
    "        # plot the updated value of x_bar\n",
    "        plt.plot(x_bar, f(x_bar), 'o', label=\"Iteration \"+str(i+1))\n",
    "    plt.axvline(x=x_bar, ls=':', color='r', linewidth=2, alpha=0.5)\n",
    "\n",
    "    print(\"Final value of x =  \", x_bar)\n",
    "    print(\"Final value of f(x)=\", f(x_bar))\n",
    "    print(\"Final value of the gradient of f(x)=\", grad_f(x_bar))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylim(ylim)\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.axvline(x=0, color='k')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da854d2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f155be37dbe4c1a9ca50a776db17fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='num iters', max=10, min=1), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def newtons_method_3(n_iters):\n",
    "    return newtons_method_w_line_search(n_iters, x_bar=1.0, ylim=[-4, 4])\n",
    "    \n",
    "interact(newtons_method_3, n_iters=IntSlider(value=1, min=1, max=10, step=1, description='num iters'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb80a1-e46d-4d9c-ba8c-2a08752c7054",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Descent direction\n",
    "* After the first iteration the search gets stuck around zero\n",
    "* The quadratic approximation of $f(x)$ around zero has negative curvature: $\\nabla^2 f(x) < 0$\n",
    "* This implies that Netwon's step $\\Delta x = -\\nabla^2 f(x)^{-1} \\, \\nabla f(x)$ points in the same direction as $\\nabla f(x)$\n",
    "* In other words, $\\Delta x$ is an **ascent** direction, whereas we want a **descent** direction\n",
    "* How can we fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5760019-ef1e-4ebf-85dc-f0236c64d95b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Hessian regularization\n",
    "* To ensure that Newton's step gives us a **descent** direction we can regularize the Hessian $H$ so that $H>0$\n",
    " $$H = \\nabla^2 f(x) + \\lambda \\, I$$\n",
    "* where $\\lambda>0$ is a regularization parameter that should be sufficiently large to ensure $H > 0$\n",
    "    * We could find $\\lambda$ by computing the minimum eigenvalue of $\\nabla^2 f(x)$, call it $e_{min}$, and then setting $\\lambda = \\epsilon + \\max(-e_{min}, 0)$, with $\\epsilon>0$ being a small positive value\n",
    "* Newton's step then becomes:\n",
    "  $$ \\Delta x = -H^{-1} \\, \\nabla f(x)$$\n",
    "* If $H>0$ then $\\Delta x$ is guaranteed to be a descent direction because:\n",
    "  $$ \\nabla f(x)^T \\, \\Delta x = - \\nabla f(x)^T \\, H^{-1} \\, \\nabla f(x) < 0 \\qquad \\forall x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13dbb8a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def newtons_method_w_line_search_and_regul(n_iters, eps, x_bar, ylim):\n",
    "    # plot the function f(x)\n",
    "    X = np.linspace(-3., 3., 100)\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    plt.plot(X, f(X), linewidth=3, label=\"f(x)\")\n",
    "\n",
    "    # perform n_iters iterations of Newton's method\n",
    "    plt.plot(x_bar, f(x_bar), 'o', label=\"Initial guess\")\n",
    "    for i in range(n_iters):\n",
    "        H = hess_f(x_bar) # compute Hessian\n",
    "        # since the Hessian is a scalar it is equal to its unique eigenvalue\n",
    "        lmbda = eps + max(-H, 0)\n",
    "        H += lmbda # regularize the Hessian\n",
    "        # at the last iteration plot 2-nd order Taylor expansion of f(x) around x_bar\n",
    "        if(i==n_iters-1):\n",
    "            plt.plot(X, f(x_bar) + grad_f(x_bar)*(X-x_bar) + 0.5*H*((X-x_bar)**2), alpha=0.5, linewidth=2)\n",
    "        delta_x = - grad_f(x_bar) / H # compute Newton's step\n",
    "        # implement line search\n",
    "        alpha = 1.0\n",
    "        x_try = x_bar + alpha*delta_x\n",
    "        while( f(x_try)>= f(x_bar)):\n",
    "            alpha *= 0.5 # reduce alpha\n",
    "            # compute new value of x\n",
    "            x_try = x_bar + alpha*delta_x\n",
    "            # stop line search if alpha is too small to avoid looping forever\n",
    "            if(alpha<1e-6):\n",
    "                break\n",
    "        print(f\"Iter {i}, H={H:.3f}, alpha={alpha}\")\n",
    "        x_bar = x_try\n",
    "        \n",
    "        # plot the updated value of x_bar\n",
    "        plt.plot(x_bar, f(x_bar), 'o', label=\"Iteration \"+str(i+1))\n",
    "    plt.axvline(x=x_bar, ls=':', color='r', linewidth=2, alpha=0.5)\n",
    "\n",
    "    print(\"Final value of x =  \", x_bar)\n",
    "    print(\"Final value of f(x)=\", f(x_bar))\n",
    "    print(\"Final value of the gradient of f(x)=\", grad_f(x_bar))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylim(ylim)\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.axvline(x=0, color='k')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0259b5e8-d59d-40dc-b3b9-f2bbc232ee67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65906509665c4407beddc29ffc0c87f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='num iters', max=10, min=1), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def newtons_method_4(n_iters):\n",
    "    return newtons_method_w_line_search_and_regul(n_iters, eps=1e-4, x_bar=1.0, ylim=[-5, 4])\n",
    "    \n",
    "interact(newtons_method_4, n_iters=IntSlider(value=1, min=1, max=10, step=1, description='num iters'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f659d-b502-4a63-8de4-6cf9b752cf78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "* Newton's method exploits second-order derivatives to ensure **faster convergence** than gradient descent on ill-conditioned functions\n",
    "* **Line search** is introduced to ensure monotonic improvement at each iteration\n",
    "* **Hessian regularization** is introduced to ensure improvement is possible even with negative-definite Hessian\n",
    "* These ideas can be extended to **constrained** optimization problems, which are subject to equality and inequality constraints"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
