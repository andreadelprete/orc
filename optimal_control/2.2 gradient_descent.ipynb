{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83db7d68-b5ea-45bd-9f05-0b77d8d1931e",
   "metadata": {},
   "source": [
    "# Gradient-based Numerical Optimization\n",
    "\n",
    "We want to minimize a differentiable function $f(x): \\mathbb{R}^n \\rightarrow \\mathbb{R}$ with respect to its multi-dimensional input vector $x \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$ \\min_x f(x) $$\n",
    "\n",
    "Let us assume we can efficiently compute the gradient of $f$, that is:\n",
    "\n",
    "$$ \\nabla f = ( \\partial_{x_0} f, \\partial_{x_1} f, \\dots) \\in \\mathbb{R}^n $$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "A simple way to find a local minimum of $f$ might be with an iterative algorithm, where at each iteration we update $x$ in the negative gradient direction by a (generally small) factor $\\eta>0$:\n",
    "\n",
    "$$ x_{k+1} = x_k - \\eta \\nabla f(x_k)$$\n",
    "\n",
    "The learning rate $\\eta$ determines how fast we move along the gradient direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea18ce-67a9-4bc2-a4f2-8a8f1f4d44b3",
   "metadata": {},
   "source": [
    "# 1. Gradient Descent on a Simple Convex Function\n",
    "\n",
    "We start with a simple quadratic function:\n",
    "\n",
    "$$ f(x) = (x - 3)^2 $$\n",
    "\n",
    "- Convex and smooth\n",
    "- Global minimum at $x = 3$\n",
    "- Gradient: $f'(x) = 2(x - 3)$\n",
    "\n",
    "Use the slider to explore different values of the learning rate $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fadf2ac-5aef-4721-9fc3-7bb73af97fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa08100c0b54d1a90e3946986242d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='η', max=1.1, min=0.01, step=0.01), Output()), _dom_c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def gradient_descent_convex(eta=0.1):\n",
    "    f = lambda x: (x - 3)**2\n",
    "    df = lambda x: 2*(x - 3)\n",
    "    x = 0\n",
    "    history = [x]\n",
    "    for _ in range(30):\n",
    "        x -= eta * df(x)\n",
    "        history.append(x)\n",
    "\n",
    "    x_vals = np.linspace(-1, 7, 200)\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.subplots(1,2)\n",
    "    ax[0].plot(x_vals, f(x_vals), label='f(x)=(x-3)^2')\n",
    "    ax[0].scatter(history, f(np.array(history)), c='red', label='Iterations')\n",
    "    ax[0].set_title(f'Convergence with Learning Rate η={eta}')\n",
    "    ax[0].set_xlabel('x'); ax[0].set_ylabel('f(x)')\n",
    "    ax[0].legend(); ax[0].grid(True)\n",
    "\n",
    "    ax[1].plot(f(np.array(history)))\n",
    "    ax[1].set_yscale(\"log\")\n",
    "    ax[1].set_ylabel(\"Cost function\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interact(gradient_descent_convex, eta=FloatSlider(value=0.1, min=0.01, max=1.1, step=0.01, description='η'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e60d62-0a4b-4910-8c0b-06670eef516d",
   "metadata": {},
   "source": [
    "# 2. Slow Convergence on an Ill-Conditioned Function\n",
    "\n",
    "Now we explore a 2D function:\n",
    "\n",
    "$$ f(x, y) = 100x^2 + y^2 $$\n",
    "\n",
    "This function has very different curvature along the $x$ and $y$ axes. Use the slider to adjust $\\eta$ and observe how gradient descent zig-zags in the narrow valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3484e7a-d7b8-49f1-aded-f4b5b2b4a84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291675339abe4a7fb45411a66a24576c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='η', max=0.05, min=0.001, step=0.001), Output()), _d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.gradient_descent_ill_conditioned(eta=0.05)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent_ill_conditioned(eta=0.05):\n",
    "    f = lambda x, y: 100*x**2 + y**2\n",
    "    df = lambda x, y: np.array([200*x, 2*y])\n",
    "\n",
    "    x = np.array([1.5, 1.5])\n",
    "    history = [x.copy()]\n",
    "    hystory_f = [f(*x)]\n",
    "    for _ in range(100):\n",
    "        x -= eta * df(*x)\n",
    "        history.append(x.copy())\n",
    "        hystory_f.append(f(*x))\n",
    "    history = np.array(history)\n",
    "\n",
    "    X, Y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\n",
    "    Z = f(X, Y)\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.subplots(1,2)\n",
    "    ax[0].contour(X, Y, Z, levels=np.logspace(-1, 3, 20))\n",
    "    ax[0].plot(history[:,0], history[:,1], 'ro-', label='Trajectory', alpha=0.5)\n",
    "    ax[0].set_title(f'Ill-Conditioned Function, η={eta}')\n",
    "    ax[0].set_xlabel('x'); ax[0].set_ylabel('y')\n",
    "    ax[0].legend(); ax[0].grid(True)\n",
    "\n",
    "    ax[1].plot(hystory_f)\n",
    "    ax[1].set_yscale(\"log\")\n",
    "    ax[1].set_ylabel(\"Cost function\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interact(gradient_descent_ill_conditioned, eta=FloatSlider(value=0.01, min=0.001, max=0.05, step=0.001, description='η'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c7d95e-bf8d-4ed1-a521-ee7fa36fb3d1",
   "metadata": {},
   "source": [
    "# Summary and Discussion\n",
    "\n",
    "| Case | Function | Behavior | Lesson |\n",
    "|------|-----------|-----------|---------|\n",
    "| Simple convex | $(x-3)^2$ | Converges quickly | Works well when convex and well-scaled |\n",
    "| Ill-conditioned | $100x^2 + y^2$ | Slow progress | Sensitive to scaling, motivates preconditioning |\n",
    "\n",
    "These interactive examples illustrate why we need more advanced (e.g., **second-order**) optimization methods for real-world problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
